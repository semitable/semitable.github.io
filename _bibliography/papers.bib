@inproceedings{tilbury2023revisiting,
  title = {Revisiting the Gumbel-Softmax in MADDPG},
  booktitle = {Adaptive and Learning Agents Workshop (ALA)},
  author = {Tilbury, Callum Rhys and Christianos, Filippos and Albrecht, Stefano},
  year = {2023},
  abbr = {AAMAS (ws)},
  selected = {false},

}
@inproceedings{michalski2023smac,
  title = {SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning},
  booktitle = {Multiagent Sequential Decision Making under Uncertainty Workshop (MSDM)},
  author = {Michalski, Adam and Christianos, Filippos and Albrecht, Stefano},
  year = {2023},
  abbr = {AAMAS (ws)},
  selected = {false},
}

@inproceedings{christianosSharedExperienceActorCritic2020,
  title = {Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Christianos, Filippos and Schäfer, Lukas and Albrecht, Stefano},
  year = {2020},
  volume = {33},
  pages = {10707--10717},
  publisher = {{Curran Associates, Inc.}},
  html = {https://proceedings.neurips.cc/paper/2020/hash/7967cc8e3ab559e68cc944c44b1cf3e8-Abstract.html},
  abbr = {NeurIPS},
  abstract = {Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called shared Experience Actor-Critic(SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.},
  selected = {true},
  slides = {seac_slides.pdf},
  poster = {seac_poster.pdf},
  code = {https://github.com/semitable/seac},
}
@inproceedings{christianosScalingMultiAgentReinforcement2021,
  title = {Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing},
  author = {Christianos, Filippos and Papoudakis, Georgios and Rahman, Arrasy and Albrecht, Stefano},
  year = {2021},
  url = {http://arxiv.org/abs/2102.07475},
  html = {http://proceedings.mlr.press/v139/christianos21a.html},
  abbr = {ICML},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  abstract = {Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. We propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns.},
  selected = {true},
  slides = {seps_slides.pdf},
  poster = {seps_poster.pdf},
  code = {https://github.com/semitable/seps},
}
@inproceedings{papoudakis2021benchmarking,
   title={Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
   author={Georgios * Papoudakis and Filippos * Christianos and Lukas Schäfer and Stefano Albrecht},
   booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
   year={2021},
   abbr={NeurIPS},
   eprint={2006.07869},
   archivePrefix={arXiv},
   primaryClass={cs.LG},
   url = {http://arxiv.org/abs/2006.07869},
   abstract = {Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learners, centralised training with decentralised execution, and value decomposition) in a diverse range of multi-agent learning tasks. Our results show that (1) algorithm performance depends strongly on environment properties and no algorithm learns efficiently across all learning tasks; (2) independent learners often achieve equal or better performance than more complex algorithms; (3) tested algorithms struggle to solve multi-agent tasks with sparse rewards. We report detailed empirical data, including a reliability analysis, and provide insights into the limitations of the tested algorithms.},
   eprinttype = {arxiv},
   selected = {true},
   openreview = {https://openreview.net/forum?id=cIrPX-Sn5n},
   code = {https://github.com/uoe-agents/epymarl},
}
@inproceedings{christianos2023planning,
  title = {Planning with Occluded Traffic Agents using Bi-Level Variational Occlusion Models},
  author={Filippos Christianos and Peter Karkus and Boris Ivanovic and Stefano Albrecht and Marco Pavone},
  year = {2023},
  booktitle = {IEEE International Conference on Robotics and Automation},
  eprint = {2210.14584},
  selected = {true},
  abstract = {Reasoning with occluded traffic agents is a significant open challenge for planning for autonomous vehicles. Recent deep learning models have shown impressive results for predicting occluded agents based on the behaviour of nearby visible agents; however, as we show in experiments, these models are difficult to integrate into downstream planning. To this end, we propose Bi-level Variational Occlusion Models (BiVO), a two-step generative model that first predicts likely locations of occluded agents, and then generates likely trajectories for the occluded agents. In contrast to existing methods, BiVO outputs a trajectory distribution which can then be sampled from and integrated into standard downstream planning. We evaluate the method in closed-loop replay simulation using the real-world nuScenes dataset. Our results suggest that BiVO can successfully learn to predict occluded agent trajectories, and these predictions lead to better subsequent motion plans in critical scenarios.},
  abbr = {ICRA}
}
@inproceedings{christianosEfficientMulticriteriaCoalition2017,
  title = {Efficient Multi-Criteria Coalition Formation Using Hypergraphs (with Application to the V2G Problem)},
  booktitle = {Multi-Agent Systems and Agreement Technologies},
  author = {Christianos, Filippos and Chalkiadakis, Georgios},
  editor = {Criado Pacheco, Natalia and Carrascosa, Carlos and Osman, Nardine and Julián Inglada, Vicente},
  year = {2017},
  pages = {92--108},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-59294-7_9},
  abbr = {EUMAS},
  abstract = {This paper proposes, for the first time in the literature, the use of hypergraphs for the efficient formation of effective coalitions. We put forward several formation methods that build on existing hypergraph pruning, transversal, and clustering algorithms, and exploit the hypergraph structure to identify agents with desirable characteristics. Our approach allows the near-instantaneous formation of high quality coalitions, adhering to multiple stated quality requirements. Moreover, our methods are shown to scale to dozens of thousands of agents within fractions of a second; with one of them scaling to even millions of agents within seconds. We apply our approach to the problem of forming coalitions to provide (electric) vehicle-to-grid (V2G) services. Ours is the first approach able to deal with large-scale, real-time coalition formation for the V2G problem, while taking multiple criteria into account for creating the electric vehicle coalitions.},
  file = {/home/almak/Zotero/storage/8QBJETBI/Christianos and Chalkiadakis - 2017 - Efficient Multi-criteria Coalition Formation Using.pdf},
  isbn = {978-3-319-59294-7},
  keywords = {Capacity Goals,Coalition Formation (CF),Hypergraph Clustering,Minimal Transversals,Optimal Coalition Structure},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@inproceedings{christianosEmployingHypergraphsEfficient2016,
  title = {Employing Hypergraphs for Efficient Coalition Formation with Application to the V2G Problem},
  booktitle = {Proceedings of the Twenty-Second European Conference on Artificial Intelligence},
  author = {Christianos, Filippos and Chalkiadakis, Georgios},
  year = {2016},
  pages = {1604--1605},
  publisher = {{IOS Press}},
  location = {{NLD}},
  doi = {10.3233/978-1-61499-672-9-1604},
  url = {https://doi.org/10.3233/978-1-61499-672-9-1604},
  abbr = {ECAI},
  abstract = {This paper proposes, for the first time in the literature, the use of hypergraphs for the efficient formation of effective coalitions. We put forward several formation methods that build on existing hypergraph algorithms, and exploit hypergraph structure to identify agents with desirable characteristics. Our approach allows the near-instantaneous formation of high quality coalitions, while adhering to multiple stated requirements regarding coalition quality. Moreover, our methods are shown to scale to dozens of thousands of agents within fractions of a second; with one of them scaling to even millions of agents within seconds. We apply our approach to the problem of forming coalitions to provide (electric) vehicle-to-grid (V2G) services. Ours is the first approach able to deal with large-scale, realtime coalition formation for the V2G problem, while taking multiple criteria into account for creating electric vehicle coalitions.},
  isbn = {978-1-61499-671-2},
  series = {{{ECAI}}'16}
}


@article{doi:10.1080/17512549.2020.1835712,
  title = {A Low-Complexity Non-Intrusive Approach to Predict the Energy Demand of Buildings over Short-Term Horizons},
  author = {Panagopoulos, Athanasios Aris and Christianos, Filippos and Katsigiannis, Michail and Mykoniatis, Konstantinos and Pritoni, Marco and Panagopoulos, Orestis P. and Peffer, Therese and Chalkiadakis, Georgios and Culler, David E. and Jennings, Nicholas R. and Timothy Lipman},
  year = {2020},
  journaltitle = {Advances in Building Energy Research},
  volume = {0},
  pages = {1--12},
  publisher = {{Taylor \& Francis}},
  html = {https://doi.org/10.1080/17512549.2020.1835712},
  url = {https://doi.org/10.1080/17512549.2020.1835712},
  abbr = {ABER},
  abstract = {Reliable, non-intrusive, short-term (of up to 12 h ahead) prediction of a building's energy demand is a critical component of intelligent energy management applications. A number of such approaches have been proposed over time, utilizing various statistical and, more 
  , machine learning techniques, such as decision trees, neural networks and support vector machines. Importantly, all of these works barely outperform simple seasonal auto-regressive integrated moving average models, while their complexity is significantly higher. In this work, we propose a novel low-complexity non-intrusive approach that improves the predictive accuracy of the state-of-the-art by up to ∼ 10 \% . The backbone of our approach is a K-nearest neighbours search method, that exploits the demand pattern of the most similar historical days, and incorporates appropriate time-series pre-processing and easing. In the context of this work, we evaluate our approach against state-of-the-art methods and provide insights on their performance.},
}


@online{papoudakisDealingNonStationarityMultiAgent2019,
  title = {Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning},
  author = {Papoudakis, Georgios and Christianos, Filippos and Rahman, Arrasy and Albrecht, Stefano},
  year = {2019-06-11},
  url = {http://arxiv.org/abs/1906.04737},
  abbr = {Preprint},
  abstract = {Recent developments in deep reinforcement learning are concerned with creating decision-making agents which can perform well in various complex domains. A particular approach which has received increasing attention is multi-agent reinforcement learning, in which multiple agents learn concurrently to coordinate their actions. In such multi-agent environments, additional learning problems arise due to the continually changing decision-making policies of agents. This paper surveys recent works that address the non-stationarity problem in multi-agent deep reinforcement learning. The surveyed methods range from modifications in the training procedure, such as centralized training, to learning representations of the opponent's policy, meta-learning, communication, and decentralized learning. The survey concludes with a list of open problems and possible lines of future research.},
  archivePrefix = {arXiv},
  eprint = {1906.04737},
  eprinttype = {arxiv},
}

@inproceedings{papoudakisLocalInformationOpponent2021,
  title = {Agent Modelling under Partial Observability for Deep Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Papoudakis, Georgios and Christianos, Filippos and Albrecht, Stefano},
  year = {2021},
  url = {http://arxiv.org/abs/2006.09447},
  abbr = {NeurIPS},
  code = {https://github.com/uoe-agents/LIAM},
  abstract = {Modelling the behaviours of other agents (opponents) is essential for understanding how agents interact and making effective decisions. Existing methods for opponent modelling commonly assume knowledge of the local observations and chosen actions of the modelled opponents, which can significantly limit their applicability. We propose a new modelling technique based on variational autoencoders, which are trained to reconstruct the local actions and observations of the opponent based on embeddings which depend only on the local observations of the modelling agent (its observed world state, chosen actions, and received rewards). The embeddings are used to augment the modelling agent's decision policy which is trained via deep reinforcement learning; thus the policy does not require access to opponent observations. We provide a comprehensive evaluation and ablation study in diverse multi-agent tasks, showing that our method achieves comparable performance to an ideal baseline which has full access to opponent's information, and significantly higher returns than a baseline method which does not use the learned embeddings.},
  openreview = {https://openreview.net/forum?id=QcwJmp1sTnk}
}

@inproceedings{rahmanOpenAdHoc2021,
  title = {Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning},
  author = {Rahman, Arrasy and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano},
  url = {http://arxiv.org/abs/2006.10412},
  abbr = {ICML},
  year = {2021},
  html = {http://proceedings.mlr.press/v139/rahman21a.html},
  code = {https://github.com/uoe-agents/GPL},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  abstract = {Ad hoc teamwork is the challenging problem of designing an autonomous agent which can adapt quickly to collaborate with previously unknown teammates. Prior work in this area has focused on closed teams in which the number of agents is fixed. In this work, we consider open teams by allowing agents of varying types to enter and leave the team without prior notification. Our solution builds on graph neural networks to learn agent models and joint action-value decompositions under varying team sizes, which can be trained with reinforcement learning using a discounted returns objective. We demonstrate empirically that our approach effectively models the impact of other agents actions on the controlled agent's returns to produce policies which can robustly adapt to dynamic team composition and is able to effectively generalize to larger teams than were seen during training.},
 }

@inproceedings{schaefer2021decoupling,
   title={Decoupling Exploration and Exploitation in Reinforcement Learning},
   author={Lukas Schäfer and Filippos Christianos and Josiah Hanna and Stefano Albrecht},
   booktitle={International Conference on Autonomous Agents and Multiagent Systems},
   year={2022},
   url = {https://arxiv.org/abs/2107.08966},
   abbr = {AAMAS},
   abstract = {Intrinsic rewards can improve exploration in reinforcement learning, but the exploration process may suffer from instability caused by non-stationary reward shaping and strong dependency on hyperparameters. In this work, we introduce Decoupled RL (DeRL) as a general framework which trains separate policies for intrinsically-motivated exploration and exploitation. Such decoupling allows DeRL to leverage the benefits of intrinsic rewards for exploration while demonstrating improved robustness and sample efficiency. We evaluate DeRL algorithms in two sparse-reward environments with multiple types of intrinsic rewards. Our results show that DeRL is more robust to varying scale and rate of decay of intrinsic rewards and converges to the same evaluation returns than intrinsically-motivated baselines in fewer interactions. Lastly, we discuss the challenge of distribution shift and show that divergence constraint regularisers can successfully minimise instability caused by divergence of exploration and exploitation policies.},
   eprint={2107.08966},
   eprinttype = {arxiv},
}

@article{albrecht2022aic,
   author = {Ahmed, Ibrahim H. and Brewitt, Cillian and Carlucho, Ignacio and Christianos, Filippos and Dunion, Mhairi and Fosong, Elliot and Garcin, Samuel and Guo, Shangmin and Gyevnar, Balint and McInroe, Trevor and Papoudakis, Georgios and Rahman, Arrasy and Schäfer, Lukas and Tamborski, Massimiliano and Vecchio, Giuseppe and Wang, Cheng and Albrecht, Stefano},
   title = {Deep Reinforcement Learning for Multi-Agent Interaction},
   journaltitle  = {AI Communications, Special Issue on Multi-Agent Systems Research in the UK},
   year = {2022},
   abbr = {AIC},
   url = {https://arxiv.org/abs/2208.01769},
   eprint={2208.01769},
   eprinttype = {arxiv},

   abstract = {The development of autonomous agents which can interact with other agents to accomplish a given task is a core area of research in artificial intelligence and machine learning. Towards this goal, the Autonomous Agents Research Group develops novel machine learning algorithms for autonomous systems control, with a specific focus on deep reinforcement learning and multi-agent reinforcement learning. Research problems include scalable learning of coordinated agent policies and inter-agent communication; reasoning about the behaviours, goals, and composition of other agents from limited observations; and sample-efficient learning based on intrinsic motivation, curriculum learning, causal inference, and representation learning. This article provides a broad overview of the ongoing research portfolio of the group and discusses open problems for future directions.},
}
