<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Filippos Christianos</title> <meta name="author" content="Filippos Christianos"> <meta name="description" content="Personal website of Filippos Christianos. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://semitable.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%66%69%6C%69%70%70%6F%73.%63%68%72%69%73%74%69%61%6E%6F%73@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=q09VRMkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/semitable" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/fchristianos" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/f_christianos" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item"><a class="nav-link" href="/assets/pdf/fchristianos_cv.pdf" target="_blank">cv<i class="fas fa-download"></i></a></li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Filippos</span> Christianos </h1> <p class="desc">Noah's Ark Lab, Huawei, London, UK.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>Previously at <a href="https://www.ed.ac.uk/informatics" rel="external nofollow noopener" target="_blank">University of Edinburgh</a> and <a href="https://nvr-avg.github.io/" rel="external nofollow noopener" target="_blank">NVIDIA Research</a>.</p> <p>I completed my PhD in the <a href="https://www.edinburgh-robotics.org/" rel="external nofollow noopener" target="_blank">CDT for Robotics and Autonomous Agents</a>, advised by <a href="https://agents.inf.ed.ac.uk/stefano-albrecht/" rel="external nofollow noopener" target="_blank">Stefano Albrecht</a> in the (<a href="https://www.ed.ac.uk/informatics" rel="external nofollow noopener" target="_blank">University of Edinburgh</a>) as a member of the <a href="https://agents.inf.ed.ac.uk/" rel="external nofollow noopener" target="_blank">Autonomous Agents Research Group</a>.</p> <p>I am working on Multi-Agent Deep Reinforcement Learning. I am co-author of the <a href="http://www.marl-book.com" rel="external nofollow noopener" target="_blank">Multi-Agent Reinforcement Learning: Foundations and Modern Approaches</a> textbook which serves as an introduction to MARL. Online version of the book is available for free! I am also the maintainer of the book‚Äôs code base found here: <a href="https://github.com/marl-book/fast-marl" rel="external nofollow noopener" target="_blank">fast-marl</a>.</p> <p>I also authored and maintain the <a href="https://www.github.com/semitable/robotic-warehouse" rel="external nofollow noopener" target="_blank">Multi-Robot Warehouse</a> environment for multi-agent RL research, and the Python version of <a href="https://www.github.com/semitable/lb-foraging" rel="external nofollow noopener" target="_blank">Level-based Foraging</a>. Finally, I am a co-author of <a href="https://github.com/uoe-agents/epymarl" rel="external nofollow noopener" target="_blank">E-PyMARL</a>, a library for MARL which has been widely used by the community.</p> <p>Keywords: Machine Learning, Deep Reinforcement Learning (RL), Multi-agent Systems, Exploration in RL.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jan 1, 2024</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> <a href="https://arxiv.org/abs/2309.16347" rel="external nofollow noopener" target="_blank">‚ÄúIntrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks ‚Äú</a> has been accepted in ICRA! It discusses how LLMs can be used to guide exploration in long-horizon, sparse-reward tasks. </td> </tr> <tr> <th scope="row">Oct 29, 2023</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Our paper <a href="https://arxiv.org/abs/2209.14344" rel="external nofollow noopener" target="_blank">‚ÄúPareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning‚Äù</a> now accepted in TMLR! Pareto-AC learns Pareto-optimal Equilibria in many MARL environments and reaches new sota results. </td> </tr> <tr> <th scope="row">May 29, 2023</th> <td> üìñ: Pre-print version of our book on MARL was just released! Find it here: <a href="http://www.marl-book.com" rel="external nofollow noopener" target="_blank">Multi-Agent Reinforcement Learning: Foundations and Modern Approaches</a> </td> </tr> <tr> <th scope="row">Jan 18, 2023</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> My NVIDIA internship resulted in ‚Äú<a href="https://arxiv.org/pdf/2210.14584" rel="external nofollow noopener" target="_blank">Planning with Occluded Traffic Agents using Bi-Level Variational Occlusion Models</a>‚Äù which was just accepted in ICRA 2023! </td> </tr> <tr> <th scope="row">Oct 29, 2022</th> <td> Preprints of my two new papers are online! My NVIDIA internship paper on <a href="https://arxiv.org/abs/2210.14584" rel="external nofollow noopener" target="_blank">AV occlusions</a> and <a href="https://arxiv.org/abs/2209.14344" rel="external nofollow noopener" target="_blank">Pareto Actor-Critic: a new algorithm for MARL</a>. </td> </tr> <tr> <th scope="row">Jun 23, 2022</th> <td> I joined <a href="https://www.nvidia.com/en-us/research/" rel="external nofollow noopener" target="_blank">NVIDIA Research</a> for a three month internship on autonomous vehicles! </td> </tr> <tr> <th scope="row">Dec 20, 2021</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Our paper titled ‚Äú<a href="https://arxiv.org/pdf/2107.08966.pdf" rel="external nofollow noopener" target="_blank">Decoupling Exploitation and Intrinsically-Motivated Exploration in Reinforcement Learning</a>‚Äù has been accepted in AAMAS 2022! </td> </tr> <tr> <th scope="row">Sep 27, 2021</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Another paper accepted at NeurIPS 2021: <a href="https://openreview.net/forum?id=QcwJmp1sTnk" rel="external nofollow noopener" target="_blank">Agent Modelling under Partial Observability for Deep Reinforcement Learning</a>. </td> </tr> <tr> <th scope="row">Jul 29, 2021</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Our benchmarking paper for MARL has been accepted at NeurIPS 2021: <a href="https://openreview.net/forum?id=cIrPX-Sn5n" rel="external nofollow noopener" target="_blank">Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks</a>. </td> </tr> <tr> <th scope="row">May 10, 2021</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"><img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Two new papers accepted at ICML 2021: <a href="https://arxiv.org/abs/2102.07475" rel="external nofollow noopener" target="_blank">Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing</a> and <a href="https://arxiv.org/abs/2006.10412" rel="external nofollow noopener" target="_blank">Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning</a>. </td> </tr> <tr> <th scope="row">Dec 20, 2020</th> <td> <img class="emoji" title=":robot:" alt=":robot:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png" height="20" width="20"> My blog post on two new environments for MARL has just been <a href="https://agents.inf.ed.ac.uk/blog/new-environments-algorithm-multiagent-rl/" rel="external nofollow noopener" target="_blank">posted</a> in our groups webpage. </td> </tr> <tr> <th scope="row">Dec 8, 2020</th> <td> <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Our paper, <a href="https://arxiv.org/abs/2006.07169" rel="external nofollow noopener" target="_blank">Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning</a>, has been accepted and published in Neural Information Processing Systems (NeurIPS 2020). </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">TMLR</abbr></div> <div id="christianos2023pareto" class="col-sm-8"> <div class="title">Pareto Actor-Critic for Equilibrium Selection in Multi-Agent Reinforcement Learning</div> <div class="author"> <em>Christianos Filippos</em>,¬†Papoudakis Georgios,¬†and¬†Albrecht Stefano V.</div> <div class="periodical"> <em>Transactions on Machine Learning Research</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2209.14344" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=3AzqYa18ah" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">OpenReview</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>This work focuses on equilibrium selection in no-conflict multi-agent games, where we specifically study the problem of selecting a Pareto-optimal Nash equilibrium among several existing equilibria. It has been shown that many state-of-the-art multi-agent reinforcement learning (MARL) algorithms are prone to converging to Pareto-dominated equilibria due to the uncertainty each agent has about the policy of the other agents during training. To address sub-optimal equilibrium selection, we propose Pareto Actor-Critic (Pareto-AC), which is an actor-critic algorithm that utilises a simple property of no-conflict games (a superset of cooperative games): the Pareto-optimal equilibrium in a no-conflict game maximises the returns of all agents and, therefore, is the preferred outcome for all agents. We evaluate Pareto-AC in a diverse set of multi-agent games and show that it converges to higher episodic returns compared to seven state-of-the-art MARL algorithms and that it successfully converges to a Pareto-optimal equilibrium in a range of matrix games. Finally, we propose PACDCG, a graph neural network extension of Pareto-AC, which is shown to efficiently scale in games with a large number of agents.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="christianosSharedExperienceActorCritic2020" class="col-sm-8"> <div class="title">Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning</div> <div class="author"> <em>Christianos Filippos</em>,¬†Sch√§fer Lukas,¬†and¬†Albrecht Stefano</div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper/2020/hash/7967cc8e3ab559e68cc944c44b1cf3e8-Abstract.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/semitable/seac" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/seac_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/seac_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called shared Experience Actor-Critic(SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICML</abbr></div> <div id="christianosScalingMultiAgentReinforcement2021" class="col-sm-8"> <div class="title">Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing</div> <div class="author"> <em>Christianos Filippos</em>,¬†Papoudakis Georgios,¬†Rahman Arrasy,¬†and¬†Albrecht Stefano</div> <div class="periodical"> <em>In Proceedings of the 38th International Conference on Machine Learning</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v139/christianos21a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://github.com/semitable/seps" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/seps_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/seps_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. We propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="papoudakis2021benchmarking" class="col-sm-8"> <div class="title">Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks</div> <div class="author"> Papoudakis Georgios *,¬†<em>Christianos Filippos *</em>,¬†Sch√§fer Lukas,¬†and¬†Albrecht Stefano</div> <div class="periodical"> <em>In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2006.07869" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a href="https://openreview.net/forum?id=cIrPX-Sn5n" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="external nofollow noopener">OpenReview</a> <a href="https://github.com/uoe-agents/epymarl" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learners, centralised training with decentralised execution, and value decomposition) in a diverse range of multi-agent learning tasks. Our results show that (1) algorithm performance depends strongly on environment properties and no algorithm learns efficiently across all learning tasks; (2) independent learners often achieve equal or better performance than more complex algorithms; (3) tested algorithms struggle to solve multi-agent tasks with sparse rewards. We report detailed empirical data, including a reliability analysis, and provide insights into the limitations of the tested algorithms.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICRA</abbr></div> <div id="christianos2023planning" class="col-sm-8"> <div class="title">Planning with Occluded Traffic Agents using Bi-Level Variational Occlusion Models</div> <div class="author"> <em>Christianos Filippos</em>,¬†Karkus Peter,¬†Ivanovic Boris,¬†Albrecht Stefano,¬†and¬†Pavone Marco</div> <div class="periodical"> <em>In IEEE International Conference on Robotics and Automation</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.14584" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Reasoning with occluded traffic agents is a significant open challenge for planning for autonomous vehicles. Recent deep learning models have shown impressive results for predicting occluded agents based on the behaviour of nearby visible agents; however, as we show in experiments, these models are difficult to integrate into downstream planning. To this end, we propose Bi-level Variational Occlusion Models (BiVO), a two-step generative model that first predicts likely locations of occluded agents, and then generates likely trajectories for the occluded agents. In contrast to existing methods, BiVO outputs a trajectory distribution which can then be sampled from and integrated into standard downstream planning. We evaluate the method in closed-loop replay simulation using the real-world nuScenes dataset. Our results suggest that BiVO can successfully learn to predict occluded agent trajectories, and these predictions lead to better subsequent motion plans in critical scenarios.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%66%69%6C%69%70%70%6F%73.%63%68%72%69%73%74%69%61%6E%6F%73@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=q09VRMkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/semitable" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/fchristianos" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/f_christianos" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2024 Filippos Christianos. Last updated: October 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://cdn.panelbear.com/analytics.js?site=Ffn3mvdBwKS"></script> <script>window.panelbear=window.panelbear||function(){(window.panelbear.q=window.panelbear.q||[]).push(arguments)},panelbear("config",{site:"Ffn3mvdBwKS"});</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>